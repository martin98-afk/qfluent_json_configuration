import os.path
import re
import shutil
from ruamel.yaml import YAML
from pathlib import Path

from loguru import logger
from concurrent.futures import ThreadPoolExecutor, as_completed
from PyQt5.QtCore import QObject, pyqtSignal, QThreadPool

from application.tools.api_service.file_uploader import DatasetUploader
from application.tools.api_service.model_duplicate import ModelDuplicate
from application.tools.api_service.model_execute import ModelExecute
from application.tools.api_service.model_uploader import ModelUploader
from application.tools.api_service.point_search import PointSearcher
from application.tools.api_service.rtsp_search import RTSPSearcher
from application.tools.api_service.service_logger import ServiceLogger
from application.tools.api_service.service_params import ServiceParamsFetcher
from application.tools.api_service.service_reonline import ServiceReonline
from application.tools.api_service.services_search import SeviceListSearcher
from application.tools.api_service.trenddb_fectcher import TrenddbFetcher
from application.tools.database.di_env import DiEnv
from application.tools.database.di_flow import DiFlow
from application.tools.database.di_flow_param_modify import DiFlowParamsModify
from application.tools.database.di_flow_params import DiFlowParams
from application.tools.nacos.get_postgres_config import GetPostgresConfig
from application.tools.nacos.get_service_path import GetNacosServicePath
from application.tools.nacos.write_service_path import WriteNacosServicePath
from application.utils.threading_utils import Worker
from application.utils.utils import resource_path


class ParamConfigLoader(QObject):
    params_loaded = pyqtSignal()
    api_tools_loaded = pyqtSignal()
    database_tools_loaded = pyqtSignal()

    def __init__(self, param_definitions_path="default.yaml"):
        super().__init__()
        self.param_definitions_path = param_definitions_path
        # å¦‚æžœé»˜è®¤é…ç½®ä¸å­˜åœ¨ï¼Œåˆ™å¤åˆ¶ä¸€ä»½å¤‡ç”¨é…ç½®
        if not os.path.exists(param_definitions_path):
            self.restore_default_params()
        self.title = "Jsoné…ç½®å·¥å…·"
        self.threadpool = QThreadPool.globalInstance()

    def _reset_config(self):
        # è¿˜åŽŸæ‰€æœ‰é…ç½®
        self.patch_info = {}
        self.param_structure = {}
        self.init_params = {}
        self.params_type = {}
        self.require_flag = {}
        self.params_default = {}
        self.params_options = {}
        self.subchildren_default = {}
        self.model_binding_structure = {}
        self.api_tools = {}
        self.tab_names = {}
        self.tool_type_dict = {}
        self.param_templates = {}

    def restore_default_params(self):
        yaml = YAML()
        config_path = Path(self.param_definitions_path)

        if not os.path.exists(config_path):
            # è¯»å–é»˜è®¤çš„yamlé…ç½®ï¼Œå¹¶ä¿å­˜åˆ°æœ¬åœ°
            try:
                shutil.copy(resource_path("default.yaml"), "default.yaml")
            except:
                logger.error("Failed to copy default.yaml")
            self.param_definitions_path = "default.yaml"
        else:
            # ç”±äºŽapi-toolsæ ç»å¸¸æœ‰ç‰ˆæœ¬å˜åŠ¨ï¼ŒåŠ è½½è€ç‰ˆæœ¬çš„é…ç½®è‡ªåŠ¨æ›´æ–°æˆæ–°ç‰ˆ
            with config_path.open("r", encoding="utf-8") as f:
                cfg = yaml.load(f)

            # å¦‚æžœæ˜¯æ—§ç‰ˆé…ç½®ï¼Œè½¬æ¢ä¸ºæ–°ç‰ˆ
            if cfg.get("api-tools", {}).get("prefix") is not None:
                # æ—§ç‰ˆé…ç½®ï¼Œè‡ªåŠ¨è½¬æ¢ä¸ºæ–°ç‰ˆé…ç½®
                prefix = cfg["api-tools"].pop("prefix")
                host = prefix.split("//")[1].split(":")[0]
                port = prefix.split("//")[1].split(":")[1]
                api_key = cfg["api-tools"].get("api-key")
                nacos_cfg = cfg.get("api-tools", {}).get("nacos")
            else:
                # æ–°ç‰ˆé…ç½®
                nacos_cfg = cfg.get("api-tools", {}).get("nacos")
                host = cfg.get("api-tools", {}).get("å…¨å±€æœåŠ¡åœ°å€")
                port = cfg.get("api-tools", {}).get("å¹³å°ç«¯å£")
                api_key = cfg.get("api-tools", {}).get("api-key")

            default_path = Path(resource_path("default.yaml"))

            with default_path.open("r", encoding="utf-8") as f:
                new_cfg = yaml.load(f)

            if nacos_cfg is not None:
                new_cfg["api-tools"]["nacos"] = nacos_cfg
            # æ›´æ–°é»˜è®¤é…ç½®ä¸­çš„å¯¹åº”å†…å®¹
            new_cfg["api-tools"]["å…¨å±€æœåŠ¡åœ°å€"] = host
            new_cfg["api-tools"]["å¹³å°ç«¯å£"] = port
            new_cfg["api-tools"]["api-key"] = api_key
            cfg["api-tools"] = new_cfg["api-tools"]

            with config_path.open("w", encoding="utf-8") as f:
                yaml.dump(cfg, f)
                logger.info("Old version config updated successfully")

    def _read_config(self):
        yaml = YAML()
        config_path = Path(self.param_definitions_path)

        with config_path.open("r", encoding="utf-8") as f:
            cfg = yaml.load(f)

        return cfg

    # ==============================
    # âœ… å…¬å…±æŽ¥å£
    # ==============================
    def load_async(self):
        self._reset_config()
        self.load_params_async()
        self.load_tools_async()

    def load_tools_async(self):
        """å¼‚æ­¥åŠ è½½å…¨éƒ¨é…ç½®"""
        cfg = self._read_config()
        cfg = cfg.get("api-tools", cfg.get("api-search", {}))
        if "å…¨å±€æœåŠ¡åœ°å€" not in cfg:
            self.restore_default_params()

        self.global_host = cfg.pop("å…¨å±€æœåŠ¡åœ°å€", "")
        self.platform_port = cfg.pop("å¹³å°ç«¯å£", "")
        self.global_api_key = cfg.pop("api-key", "")
        if "nacos" in cfg:
            # å¯¼å…¥nacoså·¥å…·
            type_cfg = {"nacos": cfg["nacos"]}
            logger.info(f"Launching asynchronous nacos tools load!")
            worker = Worker(self._load_tools_parallel, type_cfg)
            worker.signals.finished.connect(
                lambda _: (
                    logger.info(f"nacos Tools async load finished"),
                    self.database_tools_loaded.emit(),
                )
            )
            worker.signals.error.connect(
                lambda err: logger.error(f"Async full load nacos tools error: {err}")
            )
            self.threadpool.start(worker)
        else:
            self.database_tools_loaded.emit()

        if "api" in cfg:
            # å¯¼å…¥æŽ¥å£å·¥å…·
            type_cfg = {"api": cfg["api"]}
            logger.info(f"Launching asynchronous api tools load!")
            worker = Worker(self._load_tools_parallel, type_cfg)
            worker.signals.finished.connect(
                lambda _: (
                    logger.info(f"api Tools async load finished"),
                    self.api_tools_loaded.emit(),
                )
            )
            worker.signals.error.connect(
                lambda err: logger.error(f"Async full load nacos tools error: {err}")
            )
            self.threadpool.start(worker)
        else:
            self.api_tools_loaded.emit()

    def load_params_async(self):
        """å¼‚æ­¥åŠ è½½å…¨éƒ¨é…ç½®"""
        logger.info("Launching asynchronous params load")
        worker = Worker(self.load_params)
        worker.signals.finished.connect(
            lambda _: (
                logger.info("Params async load finished"),
                self.params_loaded.emit(),
            )
        )
        worker.signals.error.connect(
            lambda err: logger.error("Async params load error: {}", err)
        )
        self.threadpool.start(worker)

    # ==============================
    # ðŸ”§ å·¥å…·åŠ è½½å‡½æ•°
    # ==============================
    def _load_tools_parallel(self, cfg: dict) -> dict:
        """å¹¶è¡ŒåŠ è½½å·¥å…·å®žä¾‹"""
        # ä¼˜å…ˆåŠ è½½postgreså·¥å…·
        logger.debug("Parallel tool load started for tools: {}", list(cfg.keys()))
        tool_list = {}
        # å¢žåŠ è¿žæŽ¥postgresæ•°æ®åº“çš„å·¥å…·
        if "nacos" in cfg:
            nacos_cfg = cfg.pop("nacos", {})
            nacos_cfg = {
                "host": f"http://{self.global_host if 'host' not in nacos_cfg else nacos_cfg['host']}:{nacos_cfg.get('port', '8849')}",
                "username": nacos_cfg.get("username", "nacos"),
                "password": nacos_cfg.get("password", "nacos"),
                "namespace": nacos_cfg.get("namespace", "sushine"),
            }
            try:
                tool_list["get_service_path"] = GetNacosServicePath(**nacos_cfg)
                tool_list["write_service_path"] = WriteNacosServicePath(**nacos_cfg)
            except:
                logger.error("Failed to load eeoptimize nacos config")
            tool_list["get_postgres_config"] = GetPostgresConfig(**nacos_cfg)
            postgres_cfg = tool_list["get_postgres_config"].call()
            # å¦‚æžœpostgresä¸­hosté…ç½®ä¸æ˜¯çœŸå®žçš„åœ°å€ï¼Œåˆ™æ£€æµ‹æ˜¯å¦æœ‰postgreså•ç‹¬é…ç½®çš„åœ°å€ï¼Œå¦åˆ™ä½¿ç”¨å…¨å±€host
            if not re.search(r"^(?:[0-9]{1,3}\.){3}[0-9]{1,3}$", postgres_cfg["host"]):
                postgres_cfg["host"] = cfg.pop("postgres-host", self.global_host)
            tool_list["di_flow"] = DiFlow(**postgres_cfg)
            tool_list["di_env"] = DiEnv(**postgres_cfg)
            tool_list["di_flow_params"] = DiFlowParams(**postgres_cfg)
            tool_list["di_flow_params_modify"] = DiFlowParamsModify(**postgres_cfg)
            self.api_tools.update(tool_list)
            return
        else:
            cfg = cfg.get("api", cfg)

        def create_searcher(tool_name, cfg_tool):
            prefix = cfg_tool.pop("prefix", f"http://{self.global_host}:{self.platform_port}")
            api_key = cfg_tool.pop("api-key", self.global_api_key)
            tool_type = cfg_tool.get("type")
            if tool_type == "point-search":
                return tool_name, tool_type, PointSearcher(prefix, api_key, **cfg_tool)
            elif tool_type == "rtsp-search":
                return tool_name, tool_type, RTSPSearcher(prefix, api_key, **cfg_tool)
            elif tool_type == "file-upload":
                return tool_name, tool_type, DatasetUploader(prefix, api_key, **cfg_tool)
            elif tool_type == "model-upload":
                return tool_name, tool_type, ModelUploader(prefix, api_key, **cfg_tool)
            elif tool_type == "model-duplicate":
                return tool_name, tool_type, ModelDuplicate(prefix, api_key, **cfg_tool)
            elif tool_type == "model-execute":
                return tool_name, tool_type, ModelExecute(prefix, api_key, **cfg_tool)
            elif tool_type == "trenddb-fetcher":
                return tool_name, tool_type, TrenddbFetcher(prefix, api_key, **cfg_tool)
            elif tool_type == "services-list":
                return tool_name, tool_type, SeviceListSearcher(prefix, api_key, **cfg_tool)
            elif tool_type == "services-params":
                return tool_name, tool_type, ServiceParamsFetcher(prefix, api_key, **cfg_tool)
            elif tool_type == "services-logs":
                return tool_name, tool_type, ServiceLogger(prefix, api_key, **cfg_tool)
            elif tool_type == "services-reonline":
                return tool_name, tool_type, ServiceReonline(prefix, api_key, **cfg_tool)
            else:
                logger.error(f"æœªçŸ¥çš„å·¥å…·ç±»åž‹: {tool_type}")

        with ThreadPoolExecutor(max_workers=10) as executor:
            future_map = {
                executor.submit(create_searcher, name, spec): name
                for name, spec in cfg.items()
            }
            for future in as_completed(future_map):
                tool_name = future_map[future]
                try:
                    name, tool_type, searcher = future.result()
                    if searcher:
                        tool_list[name] = searcher
                        self.tool_type_dict.setdefault(tool_type, []).append(name)
                except Exception as e:
                    logger.error(f"åŠ è½½å·¥å…· {tool_name} å¤±è´¥: {e}")

        self.api_tools.update(tool_list)
        return

    # ==============================
    # ðŸ“Š å‚æ•°è§£æžå‡½æ•°
    # ==============================
    def load_params(self):
        """åŒæ­¥åŠ è½½å¹¶è§£æžå‚æ•°ç»“æž„"""
        logger.info("Starting synchronous param parsing")
        try:
            if os.path.exists(self.param_definitions_path):
                cfg = self._read_config()
                self.title = cfg.get("title", self.title)
                logger.success("Loaded title: {}", self.title)
                self._load_params(cfg.get("param-structure", {}))
                self.param_templates = cfg.get("param-template", {})
                self.tab_names = cfg.get("tab-names", {})
                self.patch_info = cfg.get("version-control", {})
            else:
                logger.error(
                    "Configuration file not found: {}", self.param_definitions_path
                )
        except Exception as e:
            logger.exception("Failed to parse parameters")

    def _load_params(self, param_structure: dict):
        """å®žé™…å‚æ•°è§£æžé€»è¾‘"""
        self.param_structure = param_structure
        self.init_params = self._recursive_parse(
            param_structure, self.params_type, self.require_flag, self.params_default, self.params_options
        )

    def add_binding_model_params(self, param_structure: dict):
        self.model_binding_structure = param_structure
        self.init_params = self.init_params | self._recursive_parse(
            self.model_binding_structure, self.params_type, self.require_flag, self.params_default, self.params_options
        )

    def remove_binding_model_params(self):
        self.params_type = {}
        self.require_flag = {}
        self.params_default = {}
        self.params_options = {}
        self.subchildren_default = {}
        self.model_binding_structure = {}
        self.init_params = self._recursive_parse(
            self.param_structure, self.params_type, self.require_flag, self.params_default, self.params_options
        )

    def _recursive_parse(
            self, structure, type_dict, require_flag, default_dict, options_dict, path_prefix=""
    ):
        result = {}
        for key, node in structure.items():
            full_path = f"{path_prefix}/{key}" if path_prefix else key
            type_dict[full_path] = node.get("type", "unknown")
            require_flag[full_path] = node.get("required", False)

            if "default" in node:
                default_dict[full_path] = node["default"]
            if "options" in node:
                options_dict[full_path] = node["options"]

            if "children" in node:
                result[key] = self._recursive_parse(
                    node["children"], type_dict, require_flag, default_dict, options_dict, full_path
                )
            elif "subchildren" in node:
                result[key] = ""
                self.subchildren_default[full_path] = self._recursive_parse(
                    node["subchildren"],
                    type_dict,
                    require_flag,
                    default_dict,
                    options_dict,
                    full_path,
                )
            else:
                result[key] = node.get("default", "")
        return result

    # ==============================
    # ðŸ“¦ å·¥å…·è®¿é—®æŽ¥å£
    # ==============================
    def get_tools_by_type(self, tool_type: str):
        """æ ¹æ®ç±»åž‹èŽ·å–å·¥å…·"""
        return [
            self.api_tools.get(item) for item in self.tool_type_dict.get(tool_type, [])
        ]

    def get_tools_by_path(self, path: str):
        """æ ¹æ®è·¯å¾„èŽ·å–å·¥å…·"""
        try:
            return [
                self.api_tools.get(tool_name) for tool_name in self.params_options[path]
            ]
        except Exception as e:
            logger.error(f"èŽ·å–å·¥å…·å¤±è´¥: {str(e)}")
            return []

    def get_params_name(self):
        return [
            key
            for key, value in self.param_structure.items()
            if value.get("type") == "subgroup" and "æµ‹ç‚¹å" in value.get("subchildren")
        ]

    def get_all_upload_paths(self, structure=None, path_prefix=""):
        if structure is None:
            structure = self.model_binding_structure

        upload_paths = []

        for key, value in structure.items():
            current_path = f"{path_prefix}/{key}" if path_prefix else key

            if value.get("type") == "upload":
                upload_paths.append(current_path)
            elif value.get("children") is not None:
                upload_paths.extend(self.get_all_upload_paths(value.get("children"), current_path))

        return upload_paths

    def get_model_binding_param_no(self, path):
        names = path.split("/")
        model_name = names[0]
        component_name = names[1]
        param_name = "/".join(names[2:]) if len(names) > 3 else names[2]
        return self.model_binding_structure.get(model_name).get("children").get(component_name).get("children").get(
            param_name).get("id")

    def get_model_binding_node_no(self, path):
        names = path.split("/")
        model_name = names[0]
        component_name = names[1]
        return self.model_binding_structure.get(model_name).get("children").get(component_name).get("id")